# HyperSense Trading Agent Configuration
#
# Override in config/settings.local.yml (not committed to git)
# or environment-specific files (config/settings/production.yml)

# Trading Assets
assets:
  - BTC
  - ETH
  - SOL
  - BNB

# Trading Cycle Configuration
trading:
  cycle_interval_minutes: 5      # How often the trading cycle runs (3-15 min)
  paper_trading: true            # Enable paper trading mode (no real orders)

# Risk Management
risk:
  max_position_size: 0.05        # Max 5% of capital per trade
  min_confidence: 0.6            # Minimum confidence score to execute
  max_leverage: 10               # Maximum leverage allowed
  default_leverage: 3            # Default leverage for new positions
  max_open_positions: 5          # Maximum concurrent positions
  # Position Sizing
  max_risk_per_trade: 0.01       # 1% of capital at risk per trade
  # Risk/Reward Validation
  min_risk_reward_ratio: 2.0     # Minimum R/R ratio (2:1)
  enforce_risk_reward_ratio: true # Reject trades below min R/R (false = warn only)
  # Circuit Breaker
  max_daily_loss: 0.05           # 5% max daily loss triggers circuit breaker
  max_consecutive_losses: 3      # Consecutive losses before halt
  circuit_breaker_cooldown: 24   # Hours to wait after circuit breaker trigger

# Stop Loss / Take Profit Defaults (percentage)
defaults:
  stop_loss_pct: 0.05            # 5% stop loss
  take_profit_pct: 0.10          # 10% take profit

# Context Weights for Reasoning
# Technical indicators are the primary signal (proven, based on actual price action)
# Sentiment provides confirmation/contrarian signals
# Forecast (Prophet ML) is supplementary (not designed for volatile crypto markets)
# Whale alerts hint at smart money positioning
weights:
  technical: 0.50
  sentiment: 0.25
  forecast: 0.15
  whale_alerts: 0.10

# Macro Strategy
macro:
  refresh_interval_hours: 24     # How often to refresh macro strategy
  lookback_days: 7               # Days of data for macro analysis

# LLM Configuration (supports Anthropic, Gemini, Ollama, OpenAI)
llm:
  provider: <%= ENV.fetch('LLM_PROVIDER', 'anthropic') %>
  anthropic:
    api_key: <%= ENV.fetch('ANTHROPIC_API_KEY', '') %>
    model: <%= ENV.fetch('ANTHROPIC_MODEL', 'claude-sonnet-4-5') %>
  gemini:
    api_key: <%= ENV.fetch('GEMINI_API_KEY', '') %>
    model: <%= ENV.fetch('GEMINI_MODEL', 'gemini-2.0-flash-exp') %>
  ollama:
    api_base: <%= ENV.fetch('OLLAMA_API_BASE', 'http://localhost:11434/v1') %>
    model: <%= ENV.fetch('OLLAMA_MODEL', 'llama3') %>
  openai:
    api_key: <%= ENV.fetch('OPENAI_API_KEY', '') %>
    model: <%= ENV.fetch('OPENAI_MODEL', 'gpt-5.2') %>
  high_level:
    max_tokens: 2000
    temperature: 0.3
  low_level:
    max_tokens: 1500
    temperature: 0.2

# Hyperliquid Exchange Configuration
hyperliquid:
  testnet: true                    # Use testnet by default (safer for development)
  mainnet_url: "https://api.hyperliquid.xyz"
  testnet_url: "https://api.hyperliquid-testnet.xyz"
  retry_attempts: 3                # Number of retry attempts for failed requests
  timeout: 30                      # Request timeout in seconds
  slippage: 0.005                  # 0.5% slippage for market orders

# API Rate Limits (requests per minute)
rate_limits:
  binance: 1200
  coingecko: 50
  fear_greed: 10
  hyperliquid: 100

# Cost Tracking Configuration
# Used for on-the-fly cost calculations and net P&L reporting
costs:
  # Trading fees (Hyperliquid)
  trading:
    taker_fee_pct: 0.000450       # 0.0450% per transaction (market orders)
    maker_fee_pct: 0.000150       # 0.0150% per transaction (limit orders)
    default_order_type: taker     # Assume taker fees for market orders

  # Server costs (monthly, in USD)
  server:
    monthly_cost: 15.00

  # LLM costs (per 1 million tokens, in USD)
  # Update these when pricing changes
  llm:
    anthropic:
      claude-sonnet-4-5:
        input_per_million: 3.00
        output_per_million: 15.00
      claude-haiku-4-5:
        input_per_million: 1.00
        output_per_million: 5.00
    gemini:
      gemini-2.0-flash-exp:
        input_per_million: 0.50
        output_per_million: 3.00
      gemini-3-flash-preview:
        input_per_million: 0.50
        output_per_million: 3.00
    ollama:
      # Ollama is free (local inference)
      default:
        input_per_million: 0.00
        output_per_million: 0.00
    openai:
      gpt-5.2:
        input_per_million: 1.75
        output_per_million: 14.00
      gpt-5-mini:
        input_per_million: 0.25
        output_per_million: 2.00
